# IFT 6135: Representation Learning
# Assignment 1: Multilayer Perceptron
# Authors: Samuel Laferri√®re & Joey Litalien

import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.optim as optim
import torchvision.datasets as data
import torchvision.transforms as transforms
import matplotlib.pyplot as plt


# Model parameters
batch_size = 50
h0, h1, h2, h3 = 784, 512, 512, 10
learning_rate = 1e-2
nb_epochs = 10
root = './data'
cuda = False

# torch.manual_seed(17)

# Load MNIST dataset (normalized)
mnist = transforms.Compose([transforms.ToTensor(),
                            transforms.Normalize((0.1307,), (0.3081,))])

train_loader = torch.utils.data.DataLoader(
    data.MNIST(root, train=True, download=True, transform=mnist),
    batch_size=batch_size, shuffle=True)

test_loader = torch.utils.data.DataLoader(
    data.MNIST(root, train=False, transform=mnist),
    batch_size=1, shuffle=True)

#print(iter(train_loader).next()[0][0])

def plot(losses):
    """ Plot loss/epoch """
    plt.plot(losses, 'ro')
    plt.show()

def init_weights(m):
    """ Initialize weights """
    if isinstance(m, nn.Linear):
        m.bias.data.fill_(0)
        # m.bias.data.normal_(0,1)
        # m.weight.data.fill_(0)
        # m.weight.data.normal_(0,1)
        # m.weight.data.uniform_(0,1)
        nn.init.xavier_uniform(m.weight.data)


# MLP with 2 hidden layers
model = nn.Sequential(
            nn.Linear(h0, h1), nn.ReLU(),
            nn.Linear(h1, h2), nn.ReLU(),
            nn.Linear(h2, h3)
        )

# Initialize weights
model.apply(init_weights)

# Loss function
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

# Cuda support
if cuda:
    model = model.cuda()
    criterion = criterion.cuda()

losses = []

# Training
for epoch in range(nb_epochs):
    total_loss = 0

    # Mini-batch SGD
    for batch_idx, (x, y) in enumerate(train_loader):
        # Forward pass
        x, y = Variable(x).view(batch_size, -1), Variable(y)
        if cuda:
            x = x.cuda()
            y = y.cuda()

        # Predict
        y_pred = model(x)

        # Compute and print loss
        loss = criterion(y_pred, y)
        total_loss += loss.data[0]

        # Zero gradients, perform a backward pass, and update the weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    losses.append(total_loss / (batch_idx + 1))
    # print("Epoch %d -- Avg Loss: %f" % (epoch, losses[epoch]))
    
    # Predict on test set
    correct = 0
    for i, (x, y) in enumerate(test_loader):
        # Forward pass
        x, y = Variable(x).view(1, -1), Variable(y)
        if cuda:
            x = x.cuda()
            y = y.cuda()
        
        # Predict
        y_pred = model(x)
        if (y_pred.max(1)[1] == y).data[0]:
            correct += 1

    test_acc = correct / len(test_loader)
    #print("Test Acc: %f" % acc)

    print("Epoch: [%d | %d] Avg Loss: %f | Test Acc: %f" % \
            (epoch + 1, nb_epochs, losses[epoch], test_acc))




def predict():
    """ Evaluate model on test set """

    correct = 0

    for i, (x, y) in enumerate(test_loader):
        # Foward pass
        x, y = Variable(x).view(batch_size, -1), Variable(y)
        if cuda:
            x = x.cuda()
            y = y.cuda()
        
        # Predict
        y_pred = model(x)
        if y_pred == y:
            correct += 1

    acc = correct / len(test_loader)
    print("Accuracy: %f" % acc)
        


# plot(losses)
