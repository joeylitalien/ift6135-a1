{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this multilayer perceptron can be found in `mnist.py`. The module `utils.py` contains helper functions to load the dataset, display progress bar, plot graphs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from mnist_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build an MLP and choose the values of $h^1$ and $h^2$ such that the total number of parameters (including biases) falls within the range of $I = [0.5M, 1.0M]$. This can be achieved by choosing $h^1 = h^2 = 512$. Since MNIST samples are $28 \\times 28 = 784$ pixels, the total number of parameters is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = (28*28)*512 + 512*512 + 512*10\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is within range. We thus build the MLP with the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "layers = [784, 512, 512, 10]\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "data_filename = \"../data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the tensors via Torch data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = get_data_loaders(data_filename, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardcoded parameters used for all three initilization schemes are:\n",
    "* **Activation functions:** Rectified linear unit (ReLU)\n",
    "* **Loss function:** Cross entropy\n",
    "* **Optimizer:** Stochastic gradient descent (SGD) with learning rate `learning_rate`\n",
    "\n",
    "For each initialization scheme, we compile the model and train by keeping track of the average loss. After training, we plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "mlp_z = MNIST(layers, learning_rate, \"zeros\")\n",
    "zeros_losses = mlp_z.train(2, train_loader, [], [], len(train_loader.dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg loss / epoch\n",
    "%matplotlib inline\n",
    "plot_per_epoch(zeros_losses, \"Avg loss\", \"Training with zeros initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "mlp_n = MNIST(layers, learning_rate, \"normal\")\n",
    "normal_losses = mlp_n.train(1, train_loader, [], [], len(train_loader.dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg loss / epoch\n",
    "%matplotlib inline\n",
    "plot_per_epoch(normal_losses, \"Avg loss\", \"Training with Normal(0,1) initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "mlp_g = MNIST(layers, learning_rate, \"glorot\")\n",
    "glorot_losses = mlp_g.train(3, train_loader, [], [], len(train_loader.dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg loss / epoch\n",
    "%matplotlib inline\n",
    "plot_per_epoch(glorot_losses, \"Avg loss\", \"Training with Glorot initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MNIST(layers, learning_rate, \"glorot\")\n",
    "_, train_acc, valid_acc, _ = mlp.train(1, train_loader, valid_loader, [], len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and test accuracy / epoch\n",
    "%matplotlib inline\n",
    "plots_per_epoch([train_acc, valid_acc], [\"Train\", \"Test\"], \"Avg Loss\", \"Average loss per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train by doubling the model capacity. This is done by doubling the number of neurons at the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = (28*28)*2*512 + 2*512*512 + 512*10\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_2 = [784, 2*512, 512, 10]\n",
    "mlp_2 = MNIST(layers_2, learning_rate, \"glorot\")\n",
    "train_acc_2, _, valid_acc_2, _ = mlp_2.train(1, train_loader, valid_loader, [], len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and test accuracy / epoch\n",
    "%matplotlib inline\n",
    "plots_per_epoch([train_acc_2, valid_acc_2], [\"Train\", \"Test\"], \"Avg Loss\", \"Average loss per epoch (doubled capacity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Set Size, Generalization Gap, and Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ratio $a \\in \\{0.01, 0.02, 0.05, 0.1, 1.0\\}$, we reduce the training set to $N_a = aN$ samples, where $N= 50\\,000$. We then train using this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "a = 0.01, Na = 500\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/1\n",
      "Avg loss: 2.3368 -- Train acc: 0.1250 -- Val acc: 0.1203 -- Test acc: 0.1228 -- Gen gap 0.0022\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/1\n",
      "Avg loss: 2.2491 -- Train acc: 0.2188 -- Val acc: 0.1976 -- Test acc: 0.1970 -- Gen gap 0.0218\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/1\n",
      "Avg loss: 2.2020 -- Train acc: 0.3320 -- Val acc: 0.2983 -- Test acc: 0.3043 -- Gen gap 0.0277\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "==============================\n",
      "a = 0.02, Na = 1000\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/1\n",
      "Avg loss: 2.1075 -- Train acc: 0.5195 -- Val acc: 0.5298 -- Test acc: 0.5342 -- Gen gap -0.0147\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/1\n",
      "Avg loss: 1.9974 -- Train acc: 0.6211 -- Val acc: 0.6359 -- Test acc: 0.6322 -- Gen gap -0.0111\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/1\n",
      "Avg loss: 1.8960 -- Train acc: 0.6465 -- Val acc: 0.6834 -- Test acc: 0.6691 -- Gen gap -0.0226\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "==============================\n",
      "a = 0.05, Na = 2500\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/1\n",
      "Avg loss: 1.6700 -- Train acc: 0.7000 -- Val acc: 0.7200 -- Test acc: 0.7022 -- Gen gap -0.0022\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/1\n",
      "Avg loss: 1.3993 -- Train acc: 0.7230 -- Val acc: 0.7581 -- Test acc: 0.7367 -- Gen gap -0.0136\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/1\n",
      "Avg loss: 1.1688 -- Train acc: 0.7723 -- Val acc: 0.8027 -- Test acc: 0.7875 -- Gen gap -0.0153\n",
      "Training done! Elapsed time: 0:00:00\n",
      "\n",
      "==============================\n",
      "a = 0.10, Na = 5000\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/1\n",
      "Avg loss: 0.9254 -- Train acc: 0.8258 -- Val acc: 0.8417 -- Test acc: 0.8277 -- Gen gap -0.0020\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/1\n",
      "Avg loss: 0.7587 -- Train acc: 0.8242 -- Val acc: 0.8606 -- Test acc: 0.8481 -- Gen gap -0.0240\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/1\n",
      "Avg loss: 0.6244 -- Train acc: 0.8441 -- Val acc: 0.8709 -- Test acc: 0.8613 -- Gen gap -0.0171\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "==============================\n",
      "a = 1.00, Na = 50000\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/1\n",
      "Avg loss: 0.4359 -- Train acc: 0.9006 -- Val acc: 0.9039 -- Test acc: 0.9039 -- Gen gap -0.0032\n",
      "Training done! Elapsed time: 0:00:06\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/1\n",
      "Avg loss: 0.3271 -- Train acc: 0.9149 -- Val acc: 0.9166 -- Test acc: 0.9154 -- Gen gap -0.0005\n",
      "Training done! Elapsed time: 0:00:06\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/1\n",
      "Avg loss: 0.2843 -- Train acc: 0.9234 -- Val acc: 0.9246 -- Test acc: 0.9238 -- Gen gap -0.0003\n",
      "Training done! Elapsed time: 0:00:06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize best model so far\n",
    "mlp = MNIST(layers, learning_rate, \"glorot\")\n",
    "ratios = [0.01, 0.02, 0.05, 0.1, 1.0]\n",
    "nb_epochs = 1\n",
    "nb_trials = 3\n",
    "\n",
    "# Generalization gaps\n",
    "Ga = np.zeros((len(ratios), nb_trials, nb_epochs))\n",
    "             \n",
    "for i, a in enumerate(ratios):\n",
    "    length = int(a * len(train_loader.dataset))\n",
    "    print(\"%s\\na = %.2f, Na = %d\\n%s\" % (\"=\"*30, a, length, \"-\"*30))\n",
    "    \n",
    "    for j in range(nb_trials):\n",
    "        print(\"Iter %s\" % str(j + 1))\n",
    "        # Subsample from training set\n",
    "        Na, sub_train_loader = subsample_train(a, train_loader, batch_size)\n",
    "    \n",
    "        # Train\n",
    "        train_loss, train_acc, valid_acc, test_acc = \\\n",
    "            mlp.train(nb_epochs, sub_train_loader, valid_loader, test_loader, Na, gen_gap=True)\n",
    "            \n",
    "        # Get best validation epoch\n",
    "        best_valid = max(valid_acc)\n",
    "        max_valid_idx = valid_acc.index(best_valid)\n",
    "        \n",
    "        # Save generalization gap\n",
    "        Ga[i,j] = train_acc[max_valid_idx] - test_acc[max_valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a9afcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "means, stds = Ga.mean(1), Ga.std(1)\n",
    "labels = [\"0.01\",\"0.02\",\"0.05\",\"0.1\",\"1.\"]\n",
    "plot_error_bars(labels, means, stds, \"Ratios\", \"Avg generalization gap\",\n",
    "    \"Average generalization gap for different training set ratios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
