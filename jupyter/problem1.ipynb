{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this multilayer perceptron can be found in `mnist.py`. The module `utils.py` contains helper functions to load the dataset, display progress bar, plot graphs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from mnist import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build an MLP and choose the values of $h^1$ and $h^2$ such that the total number of parameters (including biases) falls within the range of $I = [0.5M, 1.0M]$. This can be achieved by choosing $h^1 = h^2 = 512$. Since MNIST samples are $28 \\times 28 = 784$ pixels, the total number of parameters is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = (28*28)*512 + 512*512 + 512*10\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is within range. We thus build the MLP with the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "h0, h1, h2, h3 = 784, 512, 512, 10\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "nb_epochs = 3\n",
    "data_filename = \"../data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data and initialize the model with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from pickled file\n",
    "train_data, valid_data, test_data = unpickle(data_filename)\n",
    "\n",
    "# Build data loaders for all three datasets\n",
    "# Training set\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_data, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True)\n",
    "\n",
    "# Validation set\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                    valid_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "# Test set\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    test_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardcoded parameters used for all three initilization schemes are:\n",
    "* **Activation functions:** Rectified linear unit (ReLU)\n",
    "* **Loss function:** Cross entropy\n",
    "* **Optimizer:** Stochastic gradient descent (SGD) with learning rate `learning_rate`\n",
    "\n",
    "For each initialization scheme, we compile the model and train by keeping track of the average loss. After training, we plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Avg loss: 2.3019 -- Train acc: 0.1135% \n",
      "Epoch 2/3\n",
      "Avg loss: 2.3012 -- Train acc: 0.1135% \n",
      "Epoch 3/3\n",
      "Avg loss: 2.3011 -- Train acc: 0.1135% \n",
      "Training done! Elapsed time: 0:00:19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile and train model\n",
    "# Length of the training set is passed for the progress bar\n",
    "model_z, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"zeros\")\n",
    "zero_losses = train(model_z, loss_fn, optimizer, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg loss / epoch\n",
    "%matplotlib inline\n",
    "scatter_plot(zero_losses, 'Epoch', 'Avg loss', 'Training with zeros initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"normal\")\n",
    "normal_losses = train(model_n, loss_fn, optimizer, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scatter_plot(normal_losses, 'Epoch', 'Avg loss', 'Training with Normal(0,1) initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"glorot\")\n",
    "glorot_losses = train(model_g, loss_fn, optimizer, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scatter_plot(glorot_losses, 'Epoch', 'Avg loss', 'Training with Glorot initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Set Size, Generalization Gap, and Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ratio $a \\in \\{0.01, 0.02, 0.05, 0.1, 1.0\\}$, we reduce the training set to $N_a = aN$ samples, where $N= 50\\,000$. We then train using this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best model so far\n",
    "model, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"glorot\")\n",
    "ratios = [0.01, 0.02, 0.05, 0.1, 1.0]\n",
    "nb_epochs = 100\n",
    "nb_trials = 3\n",
    "\n",
    "# Generalization gaps\n",
    "Ga = np.zeros((len(ratios), nb_trials, nb_epochs))\n",
    "             \n",
    "for i, a in enumerate(ratios):\n",
    "    print(\"%s\\na = %.2f, Na = %d\\n%s\" % (\"=\"*30, a, int(a * len(train_data)), \"-\"*30))\n",
    "    \n",
    "    for j in range(nb_trials):\n",
    "        print(\"Iter %s\" % str(j + 1))\n",
    "        # Subsample from training set\n",
    "        Na, sub_train_loader = subsample_train(model, loss_fn, optimizer, a, train_loader)\n",
    "    \n",
    "        # Train\n",
    "        train_loss, train_acc, valid_acc, test_acc = \\\n",
    "            train(model, loss_fn, optimizer, Na, sub_train_loader, valid_loader, test_loader, gen_gap=True)\n",
    "        \n",
    "        # Save generalization gap\n",
    "        Ga[i,j,:] = [r_train - r_test for r_train, r_test in zip(train_acc, test_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
