{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this multilayer perceptron can be found in `mnist.py`. The module `utils.py` contains helper functions to load the dataset, display progress bar, plot graphs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from mnist import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build an MLP and choose the values of $h^1$ and $h^2$ such that the total number of parameters (including biases) falls within the range of $I = [0.5M, 1.0M]$. This can be achieved by choosing $h^1 = h^2 = 512$. Since MNIST samples are $28 \\times 28 = 784$ pixels, the total number of parameters is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = (28*28)*512 + 512*512 + 512*10\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is within range. We thus build the MLP with the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "h0, h1, h2, h3 = 784, 512, 512, 10\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "data_filename = \"../data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the tensors via Torch data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from pickled file\n",
    "train_data, valid_data, test_data = unpickle_mnist(data_filename)\n",
    "\n",
    "# Build data loaders for all three datasets\n",
    "# Training set\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_data, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True)\n",
    "\n",
    "# Validation set\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                    valid_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "# Test set\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    test_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardcoded parameters used for all three initilization schemes are:\n",
    "* **Activation functions:** Rectified linear unit (ReLU)\n",
    "* **Loss function:** Cross entropy\n",
    "* **Optimizer:** Stochastic gradient descent (SGD) with learning rate `learning_rate`\n",
    "\n",
    "For each initialization scheme, we compile the model and train by keeping track of the average loss. After training, we plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Avg loss: 2.3019 -- Train acc: 0.1135% \n",
      "Epoch 2/3\n",
      "Avg loss: 2.3012 -- Train acc: 0.1135% \n",
      "Epoch 3/3\n",
      "Avg loss: 2.3011 -- Train acc: 0.1135% \n",
      "Training done! Elapsed time: 0:00:18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile and train model\n",
    "# Length of the training set is passed for the progress bar\n",
    "# Only training data is passed for this part\n",
    "model_z, loss_fn, optimizer = build_model(h0, h1, h2, h3, \"zeros\", learning_rate)\n",
    "zero_losses = train(model_z, loss_fn, optimizer, 3, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a9a0278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot avg loss / epoch\n",
    "%matplotlib inline\n",
    "plot_per_epoch(zero_losses, \"Avg loss\", \"Training with zeros initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n, loss_fn, optimizer = build_model(h0, h1, h2, h3, \"normal\", learning_rate)\n",
    "normal_losses = train(model_n, loss_fn, optimizer, 10, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_per_epoch(normal_losses, \"Avg loss\", \"Training with Normal(0,1) initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g, loss_fn, optimizer = build_model(h0, h1, h2, h3, \"glorot\", learning_rate)\n",
    "glorot_losses = train(model_g, loss_fn, optimizer, 3, len(train_data), train_loader, valid_loader, [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_per_epoch(glorot_losses, \"Avg loss\", \"Training with Glorot initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer = build_model(h0, h1, h2, h3, \"glorot\", learning_rate)\n",
    "_, train_acc, valid_acc, _ = train(model, loss_fn, optimizer, 10, len(train_data), train_loader, valid_loader, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_per_epoch([train_acc, valid_acc], [\"Train\", \"Test\"], \"Avg Loss\", \"Training the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train by doubling the model capacity. This is done by doubling the number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = (28*28)*2*512 + 2*512*512 + 512*10\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2, loss_fn, optimizer = build_model(h0, 2*h1, 2*h2, h3, \"glorot\", learning_rate)\n",
    "train_acc_2, _, valid_acc_2, _ = train(model_g, loss_fn, optimizer, 10, len(train_data), train_loader, valid_loader, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Set Size, Generalization Gap, and Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ratio $a \\in \\{0.01, 0.02, 0.05, 0.1, 1.0\\}$, we reduce the training set to $N_a = aN$ samples, where $N= 50\\,000$. We then train using this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best model so far\n",
    "model, loss_fn, optimizer = build_model(h0, h1, h2, h3, \"glorot\", learning_rate)\n",
    "ratios = [0.01, 0.02, 0.05, 0.1, 1.0]\n",
    "nb_epochs = 100\n",
    "nb_trials = 3\n",
    "\n",
    "# Generalization gaps\n",
    "Ga = np.zeros((len(ratios), nb_trials, nb_epochs))\n",
    "             \n",
    "for i, a in enumerate(ratios):\n",
    "    print(\"%s\\na = %.2f, Na = %d\\n%s\" % (\"=\"*30, a, int(a * len(train_data)), \"-\"*30))\n",
    "    \n",
    "    for j in range(nb_trials):\n",
    "        print(\"Iter %s\" % str(j + 1))\n",
    "        # Subsample from training set\n",
    "        Na, sub_train_loader = subsample_train(model, loss_fn, optimizer, a, train_loader)\n",
    "    \n",
    "        # Train\n",
    "        train_loss, train_acc, valid_acc, test_acc = \\\n",
    "            train(model, loss_fn, optimizer, nb_epochs, Na, sub_train_loader, valid_loader, test_loader, gen_gap=True)\n",
    "        \n",
    "        # Save generalization gap\n",
    "        Ga[i,j,:] = [r_train - r_test for r_train, r_test in zip(train_acc, test_acc)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
