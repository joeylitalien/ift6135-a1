{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this multilayer perceptron can be found in `mnist.py`. The module `utils.py` contains helper functions to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from mnist import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build an MLP and choose the values of $h^1$ and $h^2$ such that the total number of parameters (including biases) falls within the range of $I = [0.5M, 1.0M]$. This can be achieved by choosing $h^1 = h^2 = 512$. Since MNIST samples are $28 \\times 28 = 784$ pixels, the total number of parameters is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = (28*28)*512 + 512*512 + 512*10\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is within range. We thus build the MLP with the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "h0, h1, h2, h3 = 784, 512, 512, 10\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "nb_epochs = 3\n",
    "data_filename = \"../data/mnist/mnist.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data and initialize the model with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from pickled file\n",
    "train_data, valid_data, test_data = unpickle(data_filename)\n",
    "\n",
    "# Build data loaders for all three datasets\n",
    "# Training set\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_data, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True)\n",
    "\n",
    "# Validation set\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                    valid_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "# Test set\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    test_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardcoded parameters used for all three initilization schemes are:\n",
    "* **Non-linear activation function:** Rectified linear unit (ReLU)\n",
    "* **Loss function:** Cross entropy\n",
    "* **Optimizer:** Stochastic gradient descent (SGD) with learning rate `learning_rate`\n",
    "\n",
    "For each initialization scheme, we compile the model and train by keeping track of the average loss. After training, we plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "# Length of the training set is passed for the progress bar\n",
    "model_z, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"zeros\")\n",
    "zero_losses = train(model_z, loss_fn, optimizer, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg loss / epoch\n",
    "%matplotlib inline\n",
    "scatter_plot(zero_losses, 'Epoch', 'Avg loss', 'Training with zeros initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"normal\")\n",
    "normal_losses = train(model_n, loss_fn, optimizer, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scatter_plot(normal_losses, 'Epoch', 'Avg loss', 'Training with Normal(0,1) initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"glorot\")\n",
    "glorot_losses = train(model_g, loss_fn, optimizer, len(train_data), train_loader, [], [])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scatter_plot(glorot_losses, 'Epoch', 'Avg loss', 'Training with Glorot initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Set Size, Generalization Gap, and Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ratio $a \\in \\{0.01, 0.02, 0.05, 0.1, 1.0\\}$, we reduce the training set to $N_a = aN$ samples, where $N= 50\\,000$. We then train using this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "a = 0.01, Na = 500\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/3\n",
      "Avg loss: 2.3082 -- Train acc: 0.1621 -- Val acc: 0.1387 -- Test acc: 0.1446 -- Gen gap 0.0175\n",
      "Epoch 2/3\n",
      "Avg loss: 2.2424 -- Train acc: 0.2070 -- Val acc: 0.1829 -- Test acc: 0.1932 -- Gen gap 0.0139\n",
      "Epoch 3/3\n",
      "Avg loss: 2.1841 -- Train acc: 0.2871 -- Val acc: 0.2537 -- Test acc: 0.2604 -- Gen gap 0.0267\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/3\n",
      "Avg loss: 2.1516 -- Train acc: 0.3633 -- Val acc: 0.3615 -- Test acc: 0.3696 -- Gen gap -0.0063\n",
      "Epoch 2/3\n",
      "Avg loss: 2.0887 -- Train acc: 0.4883 -- Val acc: 0.4750 -- Test acc: 0.4822 -- Gen gap 0.0061\n",
      "Epoch 3/3\n",
      "Avg loss: 2.0267 -- Train acc: 0.5898 -- Val acc: 0.5687 -- Test acc: 0.5744 -- Gen gap 0.0154\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/3\n",
      "Avg loss: 1.9811 -- Train acc: 0.6348 -- Val acc: 0.6345 -- Test acc: 0.6343 -- Gen gap 0.0005\n",
      "Epoch 2/3\n",
      "Avg loss: 1.9204 -- Train acc: 0.6816 -- Val acc: 0.6711 -- Test acc: 0.6627 -- Gen gap 0.0189\n",
      "Epoch 3/3\n",
      "Avg loss: 1.8599 -- Train acc: 0.6992 -- Val acc: 0.6928 -- Test acc: 0.6809 -- Gen gap 0.0183\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "==============================\n",
      "a = 0.02, Na = 1000\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/3\n",
      "Avg loss: 1.8020 -- Train acc: 0.6943 -- Val acc: 0.7087 -- Test acc: 0.7060 -- Gen gap -0.0117\n",
      "Epoch 2/3\n",
      "Avg loss: 1.6899 -- Train acc: 0.7168 -- Val acc: 0.7197 -- Test acc: 0.7226 -- Gen gap -0.0058\n",
      "Epoch 3/3\n",
      "Avg loss: 1.5762 -- Train acc: 0.7461 -- Val acc: 0.7313 -- Test acc: 0.7317 -- Gen gap 0.0144\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/3\n",
      "Avg loss: 1.4991 -- Train acc: 0.7510 -- Val acc: 0.7816 -- Test acc: 0.7781 -- Gen gap -0.0271\n",
      "Epoch 2/3\n",
      "Avg loss: 1.3848 -- Train acc: 0.7686 -- Val acc: 0.7871 -- Test acc: 0.7793 -- Gen gap -0.0107\n",
      "Epoch 3/3\n",
      "Avg loss: 1.2873 -- Train acc: 0.7764 -- Val acc: 0.8023 -- Test acc: 0.7929 -- Gen gap -0.0165\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/3\n",
      "Avg loss: 1.2091 -- Train acc: 0.7754 -- Val acc: 0.8185 -- Test acc: 0.8111 -- Gen gap -0.0357\n",
      "Epoch 2/3\n",
      "Avg loss: 1.1267 -- Train acc: 0.7861 -- Val acc: 0.8229 -- Test acc: 0.8134 -- Gen gap -0.0273\n",
      "Epoch 3/3\n",
      "Avg loss: 1.0497 -- Train acc: 0.7939 -- Val acc: 0.8295 -- Test acc: 0.8210 -- Gen gap -0.0270\n",
      "Training done! Elapsed time: 0:00:01\n",
      "\n",
      "==============================\n",
      "a = 0.05, Na = 2500\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/3\n",
      "Avg loss: 0.9692 -- Train acc: 0.8020 -- Val acc: 0.8346 -- Test acc: 0.8232 -- Gen gap -0.0213\n",
      "Epoch 2/3\n",
      "Avg loss: 0.8476 -- Train acc: 0.8145 -- Val acc: 0.8392 -- Test acc: 0.8307 -- Gen gap -0.0163\n",
      "Epoch 3/3\n",
      "Avg loss: 0.7467 -- Train acc: 0.8281 -- Val acc: 0.8541 -- Test acc: 0.8450 -- Gen gap -0.0169\n",
      "Training done! Elapsed time: 0:00:02\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/3\n",
      "Avg loss: 0.6932 -- Train acc: 0.8234 -- Val acc: 0.8624 -- Test acc: 0.8536 -- Gen gap -0.0302\n",
      "Epoch 2/3\n",
      "Avg loss: 0.6466 -- Train acc: 0.8254 -- Val acc: 0.8669 -- Test acc: 0.8564 -- Gen gap -0.0310\n",
      "Epoch 3/3\n",
      "Avg loss: 0.6053 -- Train acc: 0.8359 -- Val acc: 0.8725 -- Test acc: 0.8626 -- Gen gap -0.0266\n",
      "Training done! Elapsed time: 0:00:02\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/3\n",
      "Avg loss: 0.5963 -- Train acc: 0.8160 -- Val acc: 0.8599 -- Test acc: 0.8530 -- Gen gap -0.0370\n",
      "Epoch 2/3\n",
      "Avg loss: 0.5576 -- Train acc: 0.8391 -- Val acc: 0.8781 -- Test acc: 0.8697 -- Gen gap -0.0307\n",
      "Epoch 3/3\n",
      "Avg loss: 0.5294 -- Train acc: 0.8473 -- Val acc: 0.8838 -- Test acc: 0.8761 -- Gen gap -0.0288\n",
      "Training done! Elapsed time: 0:00:02\n",
      "\n",
      "==============================\n",
      "a = 0.10, Na = 5000\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/3\n",
      "Avg loss: 0.4877 -- Train acc: 0.8764 -- Val acc: 0.8890 -- Test acc: 0.8835 -- Gen gap -0.0071\n",
      "Epoch 2/3\n",
      "Avg loss: 0.4583 -- Train acc: 0.8764 -- Val acc: 0.8887 -- Test acc: 0.8820 -- Gen gap -0.0056\n",
      "Epoch 3/3\n",
      "Avg loss: 0.4271 -- Train acc: 0.8835 -- Val acc: 0.8940 -- Test acc: 0.8872 -- Gen gap -0.0037\n",
      "Training done! Elapsed time: 0:00:03\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/3\n",
      "Avg loss: 0.4047 -- Train acc: 0.8871 -- Val acc: 0.8990 -- Test acc: 0.8939 -- Gen gap -0.0068\n",
      "Epoch 2/3\n",
      "Avg loss: 0.3819 -- Train acc: 0.8920 -- Val acc: 0.9015 -- Test acc: 0.8935 -- Gen gap -0.0015\n",
      "Epoch 3/3\n",
      "Avg loss: 0.3697 -- Train acc: 0.8896 -- Val acc: 0.9043 -- Test acc: 0.8962 -- Gen gap -0.0066\n",
      "Training done! Elapsed time: 0:00:03\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/3\n",
      "Avg loss: 0.3662 -- Train acc: 0.8928 -- Val acc: 0.9080 -- Test acc: 0.9002 -- Gen gap -0.0074\n",
      "Epoch 2/3\n",
      "Avg loss: 0.3519 -- Train acc: 0.8948 -- Val acc: 0.9087 -- Test acc: 0.9036 -- Gen gap -0.0088\n",
      "Epoch 3/3\n",
      "Avg loss: 0.3460 -- Train acc: 0.8875 -- Val acc: 0.8984 -- Test acc: 0.8950 -- Gen gap -0.0075\n",
      "Training done! Elapsed time: 0:00:03\n",
      "\n",
      "==============================\n",
      "a = 1.00, Na = 50000\n",
      "------------------------------\n",
      "Iter 1\n",
      "Epoch 1/3\n",
      "Avg loss: 0.3224 -- Train acc: 0.9163 -- Val acc: 0.9192 -- Test acc: 0.9142 -- Gen gap 0.0021\n",
      "Epoch 2/3\n",
      "Avg loss: 0.2805 -- Train acc: 0.9263 -- Val acc: 0.9287 -- Test acc: 0.9235 -- Gen gap 0.0028\n",
      "Epoch 3/3\n",
      "Avg loss: 0.2527 -- Train acc: 0.9326 -- Val acc: 0.9333 -- Test acc: 0.9304 -- Gen gap 0.0022\n",
      "Training done! Elapsed time: 0:00:21\n",
      "\n",
      "Iter 2\n",
      "Epoch 1/3\n",
      "Avg loss: 0.2315 -- Train acc: 0.9379 -- Val acc: 0.9400 -- Test acc: 0.9361 -- Gen gap 0.0018\n",
      "Epoch 2/3\n",
      "Avg loss: 0.2140 -- Train acc: 0.9423 -- Val acc: 0.9432 -- Test acc: 0.9376 -- Gen gap 0.0047\n",
      "Epoch 3/3\n",
      "Avg loss: 0.1987 -- Train acc: 0.9460 -- Val acc: 0.9464 -- Test acc: 0.9405 -- Gen gap 0.0055\n",
      "Training done! Elapsed time: 0:00:21\n",
      "\n",
      "Iter 3\n",
      "Epoch 1/3\n",
      "Avg loss: 0.1856 -- Train acc: 0.9485 -- Val acc: 0.9496 -- Test acc: 0.9434 -- Gen gap 0.0051\n",
      "Epoch 2/3\n",
      "Avg loss: 0.1737 -- Train acc: 0.9521 -- Val acc: 0.9519 -- Test acc: 0.9459 -- Gen gap 0.0063\n",
      "Epoch 3/3\n",
      "Avg loss: 0.1634 -- Train acc: 0.9545 -- Val acc: 0.9535 -- Test acc: 0.9479 -- Gen gap 0.0066\n",
      "Training done! Elapsed time: 0:00:21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize best model so far\n",
    "model, loss_fn, optimizer = build_model(h0, h1, h2, h3, init=\"glorot\")\n",
    "nb_epochs = 3\n",
    "ratios = [0.01, 0.02, 0.05, 0.1, 1.0]\n",
    "\n",
    "# Generalization gaps\n",
    "Ga = np.zeros((5, 3, nb_epochs))\n",
    "             \n",
    "for i, a in enumerate(ratios):\n",
    "    print(\"%s\\na = %.2f, Na = %d\\n%s\" % (\"=\"*30, a, int(a * len(train_data)), \"-\"*30))\n",
    "    \n",
    "    for j in range(3):\n",
    "        print(\"Iter %s\" % str(j + 1))\n",
    "        # Subsample from training set\n",
    "        Na, sub_train_loader = subsample_train(model, loss_fn, optimizer, a, train_loader)\n",
    "    \n",
    "        # Train\n",
    "        train_loss, train_acc, valid_acc, test_acc = \\\n",
    "            train(model, loss_fn, optimizer, Na, sub_train_loader, valid_loader, test_loader, gen_gap=True)\n",
    "        \n",
    "        # Save generalization gap\n",
    "        Ga[i,j,:] = [r_train - r_test for r_train, r_test in zip(train_acc, test_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
